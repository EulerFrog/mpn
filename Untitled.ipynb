{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee471e4b-4409-4933-8297-3c2f16a069ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurogym as nygym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import networks as nets\n",
    "import net_utils as net_utils\n",
    "import int_data as syn\n",
    "import analysis as analysis\n",
    "import context_data as context\n",
    "\n",
    "\n",
    "c_vals = ['firebrick', 'darkgreen', 'blue', 'darkorange', 'm', 'deeppink', 'r', 'gray', 'g', 'navy', \n",
    "                'y', 'purple', 'cyan', 'olive', 'skyblue', 'pink', 'tan']\n",
    "c_vals_l = ['salmon', 'limegreen', 'cornflowerblue', 'bisque', 'plum', 'pink', 'tomato', 'lightgray', 'g', 'b', \n",
    "                      'y', 'purple', 'cyan', 'olive', 'skyblue', 'pink', 'tan']\n",
    "\n",
    "def participation_ratio_vector(C):\n",
    "    \"\"\"Computes the participation ratio of a vector of variances.\"\"\"\n",
    "    return np.sum(C) ** 2 / np.sum(C*C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d5f3b4-62e8-4f1b-b217-6945e38af0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'context_data' from '/home/eulerfrog/research/mpn/context_data.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(net_utils)\n",
    "reload(nets)\n",
    "reload(syn)\n",
    "reload(analysis)\n",
    "reload(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e3d77-4991-46bb-b457-13e05e063e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_utils import xe_classifier_accuracy\n",
    "\n",
    "def init_net(net_params, verbose=True):\n",
    "    \n",
    "    # initialize net with default values\n",
    "    input_dims = [net_params['n_inputs'], net_params['n_hidden'], net_params['n_outputs']]\n",
    "\n",
    "    if net_params['netType'] in ('MPN',):\n",
    "        # netClass = nets.HebbNet\n",
    "        netClass = MultiPlasticNet\n",
    "    elif net_params['netType'] in ('MPN2',):\n",
    "        netClass = MultiPlasticNetTwo        \n",
    "        input_dims.insert(1, net_params['n_hidden'])\n",
    "    elif net_params['netType'] in ('MPN_rec',):\n",
    "        netClass = MultiPlasticNetRec   \n",
    "    else:\n",
    "        raise ValueError('netType not recognized.')\n",
    "\n",
    "    net = netClass(input_dims, verbose=verbose, MAct=net_params['MAct'])\n",
    "\n",
    "    return net\n",
    "\n",
    "def train_network(net_params, toy_params, current_net=None, save=False, save_root='', \n",
    "                  set_seed=True, verbose=True):\n",
    "    \"\"\" \n",
    "    Code to train a single network. \n",
    "    \n",
    "    OUTPUTS:\n",
    "    net: the trained network\n",
    "    toy_params: these are updated when data is generated\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the random seed for reproducibility (this affects both data generation and network)\n",
    "    if 'seed' in net_params and set_seed:\n",
    "        if net_params['seed'] is not None: \n",
    "            np.random.seed(seed=net_params['seed'])\n",
    "            torch.manual_seed(net_params['seed'])\n",
    "    \n",
    "    # Intitializes network and puts it on device\n",
    "    if net_params['cuda']:\n",
    "        if verbose: print('Using CUDA...')\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        if verbose: print('Using CPU...')\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    net = init_net(net_params, verbose=verbose)\n",
    "    net.to(device)\n",
    "\n",
    "    # Continually generates new data to train on\n",
    "    # This will iterate in loop so that it only sees each type of data a set amount of times\n",
    "    net_params['epochs'] = 0 if current_net is None else net.hist['epoch']\n",
    "    validData, validOutputMask, _ = syn.generate_data(\n",
    "        net_params['valid_set_size'], toy_params, net_params['n_outputs'], \n",
    "        verbose=False, auto_balance=False, device=device)\n",
    "    early_stop = False\n",
    "    new_thresh = True # Triggers threshold setting for first call of .fit, but turns off after first call\n",
    "\n",
    "    while not early_stop:\n",
    "        net_params['epochs'] += 10 # Number of times each example is passed to the network\n",
    "        trainData, trainOutputMask, toy_params = syn.generate_data(\n",
    "            net_params['train_set_size'], toy_params, net_params['n_outputs'], \n",
    "            verbose=False, auto_balance=False, device=device)\n",
    "        \n",
    "        early_stop = net.fit('sequence', epochs=net_params['epochs'], \n",
    "                             trainData=trainData, batchSize=net_params['batch_size'],\n",
    "                             validBatch=validData[:,:,:], learningRate=net_params['learning_rate'],\n",
    "                             newThresh=new_thresh, monitorFreq=50, \n",
    "                             trainOutputMask=trainOutputMask, validOutputMask=validOutputMask,\n",
    "                             validStopThres=net_params['accEarlyStop'], weightReg=net_params['weight_reg'], \n",
    "                             regLambda=net_params['reg_lambda'], gradientClip=net_params['gradient_clip'],\n",
    "                             earlyStopValid=net_params['validEarlyStop'], minMaxIter=net_params['minMaxIter']) \n",
    "        new_thresh = False   \n",
    "\n",
    "    return net, toy_params, net_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f278e59-9e83-405e-b102-8cfedb9b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPlasticNet(StatefulBase): \"\"\" Two-layer feedforward setup, with single multi-plastic layer followed by a readout layer.\n",
    "\n",
    "    Same architecture used in \"Neural Population Dynamics of Computing with Synaptic Modulations\"\n",
    "    \"\"\"        \n",
    "    def __init__(self, init, verbose=True, **mpnArgs):        \n",
    "        super(MultiPlasticNet, self).__init__()        \n",
    "        \n",
    "        Nx,Nh,Ny = init\n",
    "        # For readouts\n",
    "        W,b = random_weight_init([Nh,Ny], bias=True)\n",
    "        \n",
    "        self.n_inputs = Nx\n",
    "        self.n_hidden = Nh\n",
    "        self.n_outputs = Ny \n",
    "        \n",
    "        self.loss_fn = F.cross_entropy # Reductions is mean by default\n",
    "        self.acc_fn = xe_classifier_accuracy \n",
    "    \n",
    "        # Creates the input MP layer\n",
    "        self.mp_layer = MultiPlasticLayer((self.n_inputs, self.n_hidden), verbose=verbose, **mpnArgs)\n",
    "    \n",
    "        # Input layer activation\n",
    "        self.f = torch.tanh\n",
    "        # Readout layer\n",
    "        self.w2 = nn.Parameter(torch.tensor(W[0], dtype=torch.float))\n",
    "        # Readout bias is not used (easier interpretting readouts in the latter)'\n",
    "        self.register_buffer('b2', torch.zeros_like(torch.tensor(b[0])))\n",
    "    \n",
    "    def reset_state(self, batchSize=1):\n",
    "        \"\"\"\n",
    "        Resets states of all internal layer SM matrices\n",
    "        \"\"\"\n",
    "        self.mp_layer.reset_state(batchSize=batchSize)   \n",
    "    \n",
    "    def forward(self, x, debug=False):\n",
    "        \"\"\"\n",
    "        This modifies the internal state of the model (self.M). \n",
    "        Don't call twice in a row unless you want to update self.M twice!\n",
    "    \n",
    "        x.shape: [B, Nx]\n",
    "        b1.shape: [Nh]\n",
    "        w1.shape=[Nx,Nh], \n",
    "        M.shape=[B,Nh,Nx], \n",
    "    \n",
    "        \"\"\"\n",
    "        # Apply input multi-plastic layer, returns pre-activation\n",
    "        h_tilde = self.mp_layer(x, debug=False)\n",
    "        h = self.f(h_tilde)\n",
    "        \n",
    "        # M updated internally when this is called\n",
    "        self.mp_layer.update_sm_matrix(x, h)                  \n",
    "    \n",
    "        # (1, Ny) + [(B, Nh,) x (Nh, Ny) = (B, Ny)] = (B, Ny)\n",
    "        y_tilde = self.b2.unsqueeze(0) + h.squeeze(dim=2) @ torch.transpose(self.w2, 0, 1)\n",
    "        y = y_tilde\n",
    "                           \n",
    "        if debug:\n",
    "            h_tilde = h_tilde.squeeze(dim=2)\n",
    "            h = h.squeeze(dim=2)\n",
    "            return h_tilde, h, y_tilde, y, self.mp_layer.M \n",
    "        else:\n",
    "            return y   \n",
    "     \n",
    "    def evaluate(self, batch):\n",
    "        \"\"\"\n",
    "        Runs a full sequence of the given back size through the network.\n",
    "        \"\"\"\n",
    "        # Begin by resetting the state\n",
    "        self.reset_state(batchSize=batch[0].shape[0])\n",
    "    \n",
    "        out_size = torch.Size([batch[1].shape[0], batch[1].shape[1], self.n_outputs]) # [B, T, Ny]\n",
    "        out = torch.empty(out_size, dtype=torch.float, layout=batch[1].layout, device=batch[1].device)\n",
    "    \n",
    "        for time_idx in range(batch[0].shape[1]):\n",
    "            x = batch[0][:, time_idx, :] # [B, Nx]\n",
    "            out[:, time_idx] = self(x)\n",
    "    \n",
    "        return out\n",
    "        \n",
    "    @torch.no_grad()    \n",
    "    def evaluate_debug(self, batch, batchMask=None, acc=True, reset=True):\n",
    "        \"\"\" \n",
    "        Runs a full sequence of the given back size through the network, but now keeps track of all sorts of parameters\n",
    "        \"\"\"\n",
    "        B = batch[0].shape[0]\n",
    "    \n",
    "        if reset:\n",
    "            self.reset_state(batchSize=B)\n",
    "    \n",
    "        Nx = self.n_inputs\n",
    "        Nh = self.n_hidden\n",
    "        Ny = self.n_outputs\n",
    "        T = batch[1].shape[1]\n",
    "        db = {'x' : torch.empty(B,T,Nx),\n",
    "              'h_tilde' : torch.empty(B,T,Nh),\n",
    "              'h' : torch.empty(B,T,Nh),\n",
    "              'Wxb' : torch.empty(B,T,Nh),\n",
    "              'M': torch.empty(B,T,Nh,Nx),\n",
    "              'Mx' : torch.empty(B,T,Nh),\n",
    "              'y_tilde' : torch.empty(B,T,Ny),\n",
    "              'out' : torch.empty(B,T,Ny),\n",
    "              }\n",
    "        for time_idx in range(batch[0].shape[1]):\n",
    "            x = batch[0][:, time_idx, :] # [B, Nx]\n",
    "            db['x'][:,time_idx,:] = x\n",
    "    \n",
    "            (db['h_tilde'][:,time_idx], db['h'][:,time_idx,:], db['y_tilde'][:,time_idx,:], \n",
    "                db['out'][:,time_idx,:], db['M'][:,time_idx,:]) = self(x, debug=True)      \n",
    "            db['Mx'][:,time_idx,:] = torch.bmm(self.mp_layer.M, x.unsqueeze(2)).squeeze(2) \n",
    "    \n",
    "            db['Wxb'][:,time_idx] = self.mp_layer.b1.unsqueeze(0) + torch.mm(x, torch.transpose(self.mp_layer.w1, 0, 1))\n",
    "        \n",
    "        if acc:\n",
    "            db['acc'] = self.accuracy(batch, out=db['out'].to(self.w2.device), outputMask=batchMask).item()  \n",
    "                             \n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_utils import random_weight_init\n",
    "\n",
    "def convert_ngym_dataset(dataset_params, set_size=None, device='cpu', mask_type=None):\n",
    "    \"\"\"\n",
    "    This converts a neroGym dataset into one that the code can use.\n",
    "\n",
    "    Mostly just transposes the batch and sequence dimensions, then combines\n",
    "    them into a TensorDataset. Also creates a mask of all trues.\n",
    "    \"\"\"\n",
    "\n",
    "    # dataset = dataset_params['dataset']\n",
    "\n",
    "    # Creates a brand new dataset each time to make sure we are starting from the\n",
    "    # beginning of a new sequence. There is probably a better way to do this.\n",
    "    kwargs = {'dt': dataset_params['dt']}\n",
    "    dataset = ngym.Dataset(dataset_params['dataset_name'],\n",
    "                           env_kwargs=kwargs, batch_size=set_size,\n",
    "                           seq_len=dataset_params['seq_length'])\n",
    "\n",
    "    if set_size is not None:\n",
    "        dataset.batchsize = set_size # Just create as a single large batch for now\n",
    "\n",
    "    act_size = dataset.env.action_space.n\n",
    "\n",
    "    inputs, labels = dataset()\n",
    "\n",
    "    # Default in our setup (batch, seq_idx, :) so need to swap dims\n",
    "    inputs = np.transpose(inputs, axes=(1, 0, 2))\n",
    "    labels = np.transpose(labels, axes=(1, 0,))[:, :, np.newaxis]\n",
    "\n",
    "    if mask_type is None: # Mask is always just all time steps, so creates all True array\n",
    "        masks = np.ones((inputs.shape[0], inputs.shape[1], act_size))\n",
    "    elif mask_type == 'label': # Masks on when labels are nonzero\n",
    "        masks_flat = (labels > 0.0).astype(np.int32) # (B, seq_len)\n",
    "        masks = np.repeat(masks_flat, act_size, axis=-1) # (B, seq_len, act_size)\n",
    "    elif mask_type == 'no_fix': # Masks on when fixation is zero, assumes fixation is zeroth input\n",
    "        masks_flat = (inputs[:, :, 0:1] == 0.0).astype(np.int32) # (B, seq_len)\n",
    "        if np.sum(masks_flat) == 0:\n",
    "            raise ValueError('Mask is all zeros!')\n",
    "        masks = np.repeat(masks_flat, act_size, axis=-1) # (B, seq_len, act_size)\n",
    "    else:\n",
    "        raise ValueError('mask type {} not recoginized'.format(mask_type))\n",
    "\n",
    "    # Note this has to happen after mask assignment since sometimes raw inputs are used to determine mask\n",
    "    if dataset_params['convert_inputs']:\n",
    "        # If the conversion is not already generated, create it\n",
    "        if 'convert_mat' not in dataset_params:\n",
    "            W,b = random_weight_init([inputs.shape[-1], dataset_params['input_dim']], bias=True)\n",
    "            dataset_params['convert_mat'] = W[0]\n",
    "            dataset_params['convert_b'] = b[0][np.newaxis, np.newaxis, :]\n",
    "\n",
    "        inputs = np.matmul(inputs, dataset_params['convert_mat'].T) + dataset_params['convert_b']\n",
    "\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float).to(device) # inputs.shape (16, 100, 3)\n",
    "    labels = torch.from_numpy(labels).type(torch.long).to(device) # labels.shape = (16, 100, 1)\n",
    "    masks = torch.tensor(masks, dtype=torch.bool, device=device)\n",
    "\n",
    "    trainData = torch.utils.data.TensorDataset(inputs, labels)\n",
    "\n",
    "    return trainData, masks, dataset_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
